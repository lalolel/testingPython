{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Python File Handling Tutorial\n",
        "\n",
        "This notebook covers essential file operations in Python, including text files, CSV files, and JSON files.\n",
        "\n",
        "## What is File Handling?\n",
        "\n",
        "File handling allows your Python programs to:\n",
        "- **Read data** from external files\n",
        "- **Write data** to files for storage\n",
        "- **Process different file formats** (text, CSV, JSON)\n",
        "- **Persist data** between program runs\n",
        "\n",
        "## The `with` Statement\n",
        "\n",
        "Python's `with` statement is the recommended way to handle files because it:\n",
        "- **Automatically closes files** when done\n",
        "- **Handles errors gracefully** \n",
        "- **Prevents resource leaks**\n",
        "- **Makes code cleaner and safer**\n",
        "\n",
        "**Basic Syntax:**\n",
        "```python\n",
        "with open('filename.txt', 'mode') as file_variable:\n",
        "    # Work with the file\n",
        "    pass\n",
        "# File automatically closes here\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## File Modes Reference\n",
        "\n",
        "| Mode | Description | Creates New File? | Overwrites? |\n",
        "|------|-------------|-------------------|-------------|\n",
        "| `'r'` | Read only (default) | No | N/A |\n",
        "| `'w'` | Write only | Yes | Yes - completely |\n",
        "| `'a'` | Append only | Yes | No - adds to end |\n",
        "| `'r+'` | Read and write | No | No |\n",
        "| `'w+'` | Write and read | Yes | Yes - completely |\n",
        "| `'a+'` | Append and read | Yes | No - adds to end |\n",
        "\n",
        "**Important:** Always specify the mode explicitly for clarity, even though `'r'` is the default.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Reading Text Files - Basic Method\n",
        "\n",
        "The `.read()` method loads the entire file content into memory as a single string. This is perfect for small files where you need all the content at once.\n",
        "\n",
        "**When to use:**\n",
        "- Small files that fit comfortably in memory\n",
        "- When you need the entire file content as one piece\n",
        "- Processing the whole file at once\n",
        "\n",
        "**Caution:** Can use a lot of memory with large files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reading entire file content at once\n",
        "# Note: This assumes 'welcome.txt' exists in your working directory\n",
        "try:\n",
        "    with open('welcome.txt', 'r') as text_file:\n",
        "        text_data = text_file.read()\n",
        "        print(\"File contents:\")\n",
        "        print(text_data)\n",
        "        print(f\"\\nFile size: {len(text_data)} characters\")\n",
        "except FileNotFoundError:\n",
        "    print(\"File 'welcome.txt' not found. Creating a sample file...\")\n",
        "    # Create a sample file for demonstration\n",
        "    with open('welcome.txt', 'w') as text_file:\n",
        "        text_file.write(\"Welcome to Python File Handling!\\n\")\n",
        "        text_file.write(\"This is a sample text file.\\n\")\n",
        "        text_file.write(\"You can read, write, and modify files easily.\")\n",
        "    \n",
        "    # Now read the file\n",
        "    with open('welcome.txt', 'r') as text_file:\n",
        "        text_data = text_file.read()\n",
        "        print(\"File contents:\")\n",
        "        print(text_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Reading Text Files Line by Line\n",
        "\n",
        "The `.readlines()` method returns a list where each element is a line from the file. This approach is memory-efficient for large files and gives you more control over processing.\n",
        "\n",
        "**Benefits:**\n",
        "- Process files line by line\n",
        "- More memory efficient for large files\n",
        "- Easier to handle structured text data\n",
        "- Can stop processing early if needed\n",
        "\n",
        "**Note:** Each line includes the newline character (`\\n`) at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reading file line by line\n",
        "try:\n",
        "    with open('how_many_lines.txt', 'r') as lines_doc:\n",
        "        lines = lines_doc.readlines()\n",
        "        print(f\"File has {len(lines)} lines:\\n\")\n",
        "        \n",
        "        for i, line in enumerate(lines, 1):\n",
        "            print(f\"Line {i}: {line.rstrip()}\")\n",
        "            \n",
        "except FileNotFoundError:\n",
        "    print(\"File 'how_many_lines.txt' not found. Creating a sample file...\")\n",
        "    # Create a sample file with multiple lines\n",
        "    with open('how_many_lines.txt', 'w') as lines_doc:\n",
        "        lines_doc.write(\"First line of the file\\n\")\n",
        "        lines_doc.write(\"Second line with some data\\n\")\n",
        "        lines_doc.write(\"Third line for demonstration\\n\")\n",
        "        lines_doc.write(\"Fourth and final line\")\n",
        "    \n",
        "    # Now read the file line by line\n",
        "    with open('how_many_lines.txt', 'r') as lines_doc:\n",
        "        lines = lines_doc.readlines()\n",
        "        print(f\"File has {len(lines)} lines:\\n\")\n",
        "        \n",
        "        for i, line in enumerate(lines, 1):\n",
        "            print(f\"Line {i}: {line.rstrip()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternative: Iterating Directly Over File Object\n",
        "\n",
        "You can iterate directly over a file object without calling `.readlines()`. This is the most memory-efficient approach for large files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# More memory-efficient line-by-line reading\n",
        "print(\"Reading file using direct iteration:\")\n",
        "with open('how_many_lines.txt', 'r') as lines_doc:\n",
        "    for line_num, line in enumerate(lines_doc, 1):\n",
        "        print(f\"Line {line_num}: {line.strip()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Writing Text Files - Write Mode\n",
        "\n",
        "Write mode (`'w'`) creates a new file or completely overwrites an existing file. Use this when you want to start fresh with new content.\n",
        "\n",
        "**Key Points:**\n",
        "- **Creates new file** if it doesn't exist\n",
        "- **Overwrites completely** if file exists\n",
        "- **No way to recover** overwritten data\n",
        "- File is created even if you don't write anything\n",
        "\n",
        "**Use Cases:**\n",
        "- Creating new files\n",
        "- Replacing entire file contents\n",
        "- Generating reports or logs from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Writing to a file (overwrites existing content)\n",
        "with open('bad_bands.txt', 'w') as bad_bands_doc:\n",
        "    bad_bands_doc.write('The Beatles')  # Controversial opinion!\n",
        "    # Note: This completely replaces any existing content\n",
        "\n",
        "print(\"File 'bad_bands.txt' has been created/overwritten.\")\n",
        "\n",
        "# Let's read it back to confirm\n",
        "with open('bad_bands.txt', 'r') as bad_bands_doc:\n",
        "    content = bad_bands_doc.read()\n",
        "    print(f\"File contents: '{content}'\")\n",
        "\n",
        "# Writing multiple lines\n",
        "with open('good_bands.txt', 'w') as good_bands_doc:\n",
        "    good_bands_doc.write('Led Zeppelin\\n')\n",
        "    good_bands_doc.write('Pink Floyd\\n')\n",
        "    good_bands_doc.write('Queen\\n')\n",
        "    good_bands_doc.write('The Rolling Stones')\n",
        "\n",
        "print(\"\\nCreated 'good_bands.txt' with multiple lines:\")\n",
        "with open('good_bands.txt', 'r') as good_bands_doc:\n",
        "    print(good_bands_doc.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Appending to Text Files - Append Mode\n",
        "\n",
        "Append mode (`'a'`) adds new content to the end of an existing file without removing what's already there. If the file doesn't exist, it creates a new one.\n",
        "\n",
        "**Benefits:**\n",
        "- **Preserves existing content**\n",
        "- **Adds to the end** of the file\n",
        "- **Safe for logs** and cumulative data\n",
        "- **Creates file** if it doesn't exist\n",
        "\n",
        "**Common Uses:**\n",
        "- Log files\n",
        "- Adding entries to lists\n",
        "- Incremental data collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Appending to a file (preserves existing content)\n",
        "# First, let's create a file with some initial content\n",
        "with open('cool_dogs.txt', 'w') as cool_dogs_file:\n",
        "    cool_dogs_file.write('Lassie\\n')\n",
        "    cool_dogs_file.write('Beethoven\\n')\n",
        "\n",
        "print(\"Initial file contents:\")\n",
        "with open('cool_dogs.txt', 'r') as cool_dogs_file:\n",
        "    print(cool_dogs_file.read())\n",
        "\n",
        "# Now append new content\n",
        "with open('cool_dogs.txt', 'a') as cool_dogs_file:\n",
        "    cool_dogs_file.write('Air Buddy\\n')\n",
        "    cool_dogs_file.write('Scooby-Doo\\n')\n",
        "\n",
        "print(\"After appending:\")\n",
        "with open('cool_dogs.txt', 'r') as cool_dogs_file:\n",
        "    content = cool_dogs_file.read()\n",
        "    print(content)\n",
        "    print(f\"Total dogs listed: {len(content.strip().split())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Reading CSV Files - Basic Method\n",
        "\n",
        "CSV (Comma-Separated Values) files are a common format for structured data. You can read them as plain text, but Python's `csv` module provides much better tools.\n",
        "\n",
        "**CSV Format Basics:**\n",
        "- Values separated by commas (or other delimiters)\n",
        "- Usually has headers in the first row\n",
        "- Can contain text, numbers, dates, etc.\n",
        "- Widely supported by spreadsheet applications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reading CSV as plain text (basic method)\n",
        "try:\n",
        "    with open('logger.csv', 'r') as log_csv_file:\n",
        "        csv_content = log_csv_file.read()\n",
        "        print(\"Raw CSV content:\")\n",
        "        print(csv_content)\n",
        "except FileNotFoundError:\n",
        "    print(\"File 'logger.csv' not found. We'll create one in the next examples.\")\n",
        "    \n",
        "    # Create a sample CSV file\n",
        "    sample_csv = \"time,address,limit\\n08:39:37,1.227.124.181,844404\\n13:13:35,198.51.139.193,543871\"\n",
        "    with open('sample_log.csv', 'w') as sample_file:\n",
        "        sample_file.write(sample_csv)\n",
        "    \n",
        "    print(\"Created sample CSV file:\")\n",
        "    with open('sample_log.csv', 'r') as sample_file:\n",
        "        print(sample_file.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Reading CSV Files with csv.DictReader\n",
        "\n",
        "`csv.DictReader` is a powerful tool that treats each row as a dictionary, using the first row as column headers. This makes working with CSV data much more intuitive.\n",
        "\n",
        "**Advantages:**\n",
        "- **Column names as keys** - access data by header name\n",
        "- **Automatic parsing** - handles quotes, commas in data\n",
        "- **Easy data extraction** - perfect for processing specific columns\n",
        "- **Readable code** - `row['Email']` vs `row[2]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# Create a sample users.csv file for demonstration\n",
        "sample_users = \"\"\"Name,Email,Age,City\n",
        "John Doe,john@example.com,25,New York\n",
        "Jane Smith,jane@example.com,30,Los Angeles\n",
        "Bob Johnson,bob@example.com,35,Chicago\n",
        "Alice Brown,alice@example.com,28,Houston\"\"\"\n",
        "\n",
        "with open('users.csv', 'w') as users_file:\n",
        "    users_file.write(sample_users)\n",
        "\n",
        "# Now read CSV using DictReader\n",
        "list_of_email_addresses = []\n",
        "\n",
        "with open('users.csv', newline='') as users_csv:\n",
        "    user_reader = csv.DictReader(users_csv)\n",
        "    \n",
        "    print(\"CSV Headers:\", user_reader.fieldnames)\n",
        "    print(\"\\nProcessing rows:\")\n",
        "    \n",
        "    for row_num, row in enumerate(user_reader, 1):\n",
        "        print(f\"Row {row_num}: {row['Name']} ({row['Email']}) - Age {row['Age']}\")\n",
        "        list_of_email_addresses.append(row['Email'])\n",
        "\n",
        "print(f\"\\nExtracted {len(list_of_email_addresses)} email addresses:\")\n",
        "for email in list_of_email_addresses:\n",
        "    print(f\"  - {email}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. CSV Files with Custom Delimiters\n",
        "\n",
        "Not all \"CSV\" files use commas as separators. `csv.DictReader` can handle different delimiters like semicolons, tabs, or custom characters like `@`.\n",
        "\n",
        "**Common Delimiters:**\n",
        "- `,` (comma) - standard CSV\n",
        "- `;` (semicolon) - European CSV format\n",
        "- `\\t` (tab) - TSV (Tab-Separated Values)\n",
        "- `|` (pipe) - database exports\n",
        "- Custom characters as needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# Create a sample books.csv with @ delimiter\n",
        "sample_books = \"\"\"Title@Author@ISBN@Year\n",
        "1984@George Orwell@978-0451524935@1949\n",
        "To Kill a Mockingbird@Harper Lee@978-0061120084@1960\n",
        "The Great Gatsby@F. Scott Fitzgerald@978-0743273565@1925\n",
        "Pride and Prejudice@Jane Austen@978-0486284736@1813\"\"\"\n",
        "\n",
        "with open('books.csv', 'w') as books_file:\n",
        "    books_file.write(sample_books)\n",
        "\n",
        "# Read CSV with custom delimiter\n",
        "with open('books.csv', 'r') as books_csv:\n",
        "    books_reader = csv.DictReader(books_csv, delimiter='@')\n",
        "    \n",
        "    print(\"Books database:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Extract ISBN numbers using list comprehension\n",
        "    isbn_list = [book['ISBN'] for book in books_reader]\n",
        "\n",
        "# Read again to display full information (reader is exhausted after first use)\n",
        "with open('books.csv', 'r') as books_csv:\n",
        "    books_reader = csv.DictReader(books_csv, delimiter='@')\n",
        "    \n",
        "    for book in books_reader:\n",
        "        print(f\"'{book['Title']}' by {book['Author']} ({book['Year']})\")\n",
        "        print(f\"  ISBN: {book['ISBN']}\")\n",
        "        print()\n",
        "\n",
        "print(f\"Extracted {len(isbn_list)} ISBN numbers:\")\n",
        "for isbn in isbn_list:\n",
        "    print(f\"  - {isbn}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Writing CSV Files with csv.DictWriter\n",
        "\n",
        "`csv.DictWriter` is the counterpart to `DictReader` for writing CSV files. It allows you to write dictionaries as CSV rows, making it perfect for structured data output.\n",
        "\n",
        "**Key Methods:**\n",
        "- `writeheader()` - writes the column headers\n",
        "- `writerow(dict)` - writes a single dictionary as a row\n",
        "- `writerows(list_of_dicts)` - writes multiple dictionaries\n",
        "\n",
        "**Benefits:**\n",
        "- **Structured output** - ensures consistent column order\n",
        "- **Automatic formatting** - handles commas, quotes in data\n",
        "- **Header management** - easy to add column names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# Sample access log data (like server logs)\n",
        "access_log = [\n",
        "    {'time': '08:39:37', 'limit': 844404, 'address': '1.227.124.181'},\n",
        "    {'time': '13:13:35', 'limit': 543871, 'address': '198.51.139.193'},\n",
        "    {'time': '19:40:45', 'limit': 3021, 'address': '172.1.254.208'},\n",
        "    {'time': '18:57:16', 'limit': 67031769, 'address': '172.58.247.219'},\n",
        "    {'time': '21:17:13', 'limit': 9083, 'address': '124.144.20.113'},\n",
        "    {'time': '23:34:17', 'limit': 65913, 'address': '203.236.149.220'},\n",
        "    {'time': '13:58:05', 'limit': 1541474, 'address': '192.52.206.76'},\n",
        "    {'time': '10:52:00', 'limit': 11465607, 'address': '104.47.149.93'},\n",
        "    {'time': '14:56:12', 'limit': 109, 'address': '192.31.185.7'},\n",
        "    {'time': '18:56:35', 'limit': 6207, 'address': '2.228.164.197'}\n",
        "]\n",
        "\n",
        "# Define the field order for CSV columns\n",
        "fields = ['time', 'address', 'limit']\n",
        "\n",
        "# Write data to CSV file\n",
        "with open('logger.csv', 'w', newline='') as logger_csv:\n",
        "    log_writer = csv.DictWriter(logger_csv, fieldnames=fields)\n",
        "    \n",
        "    # Write the header row\n",
        "    log_writer.writeheader()\n",
        "    \n",
        "    # Write each log entry as a row\n",
        "    for line in access_log:\n",
        "        log_writer.writerow(line)\n",
        "\n",
        "print(f\"Successfully wrote {len(access_log)} log entries to 'logger.csv'\")\n",
        "\n",
        "# Verify by reading the file back\n",
        "print(\"\\nVerifying written data:\")\n",
        "with open('logger.csv', 'r') as logger_csv:\n",
        "    content = logger_csv.read()\n",
        "    print(content)\n",
        "\n",
        "# Statistical analysis of the log data\n",
        "total_requests = len(access_log)\n",
        "avg_limit = sum(entry['limit'] for entry in access_log) / total_requests\n",
        "max_limit = max(entry['limit'] for entry in access_log)\n",
        "min_limit = min(entry['limit'] for entry in access_log)\n",
        "\n",
        "print(f\"\\nLog Analysis:\")\n",
        "print(f\"Total requests: {total_requests}\")\n",
        "print(f\"Average limit: {avg_limit:,.0f}\")\n",
        "print(f\"Maximum limit: {max_limit:,}\")\n",
        "print(f\"Minimum limit: {min_limit:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Reading JSON Files\n",
        "\n",
        "JSON (JavaScript Object Notation) is a popular format for structured data, especially in web applications and APIs. Python's `json` module makes it easy to work with JSON data.\n",
        "\n",
        "**JSON Basics:**\n",
        "- **Lightweight** text format\n",
        "- **Human-readable** structure\n",
        "- **Maps to Python types** - objects→dict, arrays→list, etc.\n",
        "- **Web standard** for APIs and configuration\n",
        "\n",
        "**Python JSON Mapping:**\n",
        "- JSON object → Python dict\n",
        "- JSON array → Python list\n",
        "- JSON string → Python str\n",
        "- JSON number → Python int/float\n",
        "- JSON boolean → Python bool\n",
        "- JSON null → Python None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Create a sample JSON file\n",
        "sample_message = {\n",
        "    \"text\": \"Hello, World! This is a JSON message.\",\n",
        "    \"author\": \"Python Tutorial\",\n",
        "    \"timestamp\": \"2024-01-15T10:30:00Z\",\n",
        "    \"priority\": \"high\",\n",
        "    \"read\": False,\n",
        "    \"tags\": [\"tutorial\", \"json\", \"python\"],\n",
        "    \"metadata\": {\n",
        "        \"version\": \"1.0\",\n",
        "        \"encoding\": \"utf-8\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Write JSON to file first\n",
        "with open('message.json', 'w') as message_file:\n",
        "    json.dump(sample_message, message_file, indent=2)\n",
        "\n",
        "print(\"Created 'message.json' file\\n\")\n",
        "\n",
        "# Now read JSON from file\n",
        "with open('message.json', 'r') as message_json:\n",
        "    message = json.load(message_json)\n",
        "    \n",
        "    # Access the main text\n",
        "    print(f\"Message text: {message['text']}\")\n",
        "    \n",
        "    # Access nested data\n",
        "    print(f\"Author: {message['author']}\")\n",
        "    print(f\"Priority: {message['priority']}\")\n",
        "    print(f\"Read status: {message['read']}\")\n",
        "    \n",
        "    # Access arrays and nested objects\n",
        "    print(f\"Tags: {', '.join(message['tags'])}\")\n",
        "    print(f\"Version: {message['metadata']['version']}\")\n",
        "    \n",
        "    # Show the data type\n",
        "    print(f\"\\nPython type: {type(message)}\")\n",
        "    print(f\"Available keys: {list(message.keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Writing JSON Files\n",
        "\n",
        "Writing JSON files is straightforward with `json.dump()`. You can write any Python data structure that maps to JSON (dicts, lists, strings, numbers, booleans, None).\n",
        "\n",
        "**Common Parameters:**\n",
        "- `indent` - adds pretty formatting with indentation\n",
        "- `sort_keys` - sorts dictionary keys alphabetically\n",
        "- `ensure_ascii` - controls non-ASCII character encoding\n",
        "\n",
        "**Use Cases:**\n",
        "- Configuration files\n",
        "- API responses\n",
        "- Data exchange between applications\n",
        "- Storing complex data structures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Complex data structure to save as JSON\n",
        "data_payload = [\n",
        "    {\n",
        "        'interesting message': 'What is JSON? A web application\\'s little pile of secrets.',\n",
        "        'follow up': 'But enough talk!',\n",
        "        'timestamp': '2024-01-15T14:30:00Z',\n",
        "        'importance': 9,\n",
        "        'categories': ['json', 'standards', 'interoperability']\n",
        "    },\n",
        "    {\n",
        "        'interesting message': 'Python makes JSON handling incredibly easy with the json module.',\n",
        "        'follow up': 'Perfect for modern applications!',\n",
        "        'timestamp': '2024-01-15T14:40:00Z',\n",
        "        'importance': 7,\n",
        "        'categories': ['python', 'json', 'tutorial']\n",
        "    }\n",
        "]\n",
        "\n",
        "# Write to JSON file with pretty formatting\n",
        "with open('data.json', 'w') as data_json:\n",
        "    json.dump(data_payload, data_json, indent=2, sort_keys=True)\n",
        "\n",
        "print(f\"Successfully wrote {len(data_payload)} records to 'data.json'\")\n",
        "\n",
        "# Read back and display\n",
        "print(\"\\nFile contents:\")\n",
        "with open('data.json', 'r') as data_json:\n",
        "    content = data_json.read()\n",
        "    print(content[:300] + \"...\" if len(content) > 300 else content)\n",
        "\n",
        "# Load and analyze the data\n",
        "with open('data.json', 'r') as data_json:\n",
        "    loaded_data = json.load(data_json)\n",
        "    \n",
        "print(f\"\\nData Analysis:\")\n",
        "print(f\"Number of messages: {len(loaded_data)}\")\n",
        "avg_importance = sum(msg['importance'] for msg in loaded_data) / len(loaded_data)\n",
        "print(f\"Average importance: {avg_importance:.1f}\")\n",
        "\n",
        "# Show all unique categories\n",
        "all_categories = set()\n",
        "for msg in loaded_data:\n",
        "    all_categories.update(msg['categories'])\n",
        "print(f\"All categories: {sorted(all_categories)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Working with JSON Strings\n",
        "\n",
        "Sometimes you need to work with JSON data as strings rather than files. The `json` module provides `loads()` and `dumps()` for this purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Convert Python object to JSON string\n",
        "python_data = {\n",
        "    \"name\": \"Alice\",\n",
        "    \"age\": 30,\n",
        "    \"hobbies\": [\"reading\", \"hiking\", \"coding\"],\n",
        "    \"active\": True\n",
        "}\n",
        "\n",
        "# dumps() = \"dump string\" - converts to JSON string\n",
        "json_string = json.dumps(python_data, indent=2)\n",
        "print(\"Python object as JSON string:\")\n",
        "print(json_string)\n",
        "print(f\"Type: {type(json_string)}\")\n",
        "\n",
        "# loads() = \"load string\" - converts from JSON string\n",
        "parsed_data = json.loads(json_string)\n",
        "print(f\"\\nParsed back to Python: {parsed_data}\")\n",
        "print(f\"Type: {type(parsed_data)}\")\n",
        "print(f\"Name: {parsed_data['name']}, Age: {parsed_data['age']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## File Handling Best Practices\n",
        "\n",
        "### 1. Error Handling\n",
        "Always handle potential file errors gracefully:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import csv\n",
        "\n",
        "def safe_read_json(filename):\n",
        "    \"\"\"Safely read a JSON file with comprehensive error handling.\"\"\"\n",
        "    try:\n",
        "        with open(filename, 'r') as file:\n",
        "            data = json.load(file)\n",
        "            return data, None  # data, error\n",
        "    except FileNotFoundError:\n",
        "        return None, f\"File '{filename}' not found\"\n",
        "    except json.JSONDecodeError as e:\n",
        "        return None, f\"Invalid JSON in '{filename}': {e}\"\n",
        "    except PermissionError:\n",
        "        return None, f\"Permission denied accessing '{filename}'\"\n",
        "    except Exception as e:\n",
        "        return None, f\"Unexpected error reading '{filename}': {e}\"\n",
        "\n",
        "# Test the function\n",
        "data, error = safe_read_json('data.json')\n",
        "if error:\n",
        "    print(f\"Error: {error}\")\n",
        "else:\n",
        "    print(f\"Successfully read {len(data)} items from JSON file\")\n",
        "\n",
        "# Test with non-existent file\n",
        "data, error = safe_read_json('nonexistent.json')\n",
        "if error:\n",
        "    print(f\"Expected error: {error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. File Path Handling\n",
        "Use the `pathlib` module for robust file path operations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# Modern file path handling\n",
        "data_dir = Path('data_files')\n",
        "data_dir.mkdir(exist_ok=True)  # Create directory if it doesn't exist\n",
        "\n",
        "# Create file paths\n",
        "config_file = data_dir / 'config.json'\n",
        "log_file = data_dir / 'application.log'\n",
        "csv_file = data_dir / 'users.csv'\n",
        "\n",
        "print(f\"Config file path: {config_file}\")\n",
        "print(f\"Log file path: {log_file}\")\n",
        "print(f\"CSV file path: {csv_file}\")\n",
        "\n",
        "# Check if files exist\n",
        "print(f\"\\nFile existence:\")\n",
        "print(f\"Config exists: {config_file.exists()}\")\n",
        "print(f\"Log exists: {log_file.exists()}\")\n",
        "print(f\"CSV exists: {csv_file.exists()}\")\n",
        "\n",
        "# Create a sample config file\n",
        "config_data = {\n",
        "    \"app_name\": \"File Handler Demo\",\n",
        "    \"version\": \"1.0\",\n",
        "    \"debug\": True,\n",
        "    \"max_file_size\": 1024000\n",
        "}\n",
        "\n",
        "with open(config_file, 'w') as f:\n",
        "    json.dump(config_data, f, indent=2)\n",
        "\n",
        "print(f\"\\nCreated config file: {config_file}\")\n",
        "print(f\"File size: {config_file.stat().st_size} bytes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Working with Large Files\n",
        "For large files, process them chunk by chunk to avoid memory issues:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_large_file(filename, chunk_size=1024):\n",
        "    \"\"\"Process a large file in chunks to save memory.\"\"\"\n",
        "    try:\n",
        "        with open(filename, 'r') as file:\n",
        "            chunk_count = 0\n",
        "            total_chars = 0\n",
        "            \n",
        "            while True:\n",
        "                chunk = file.read(chunk_size)\n",
        "                if not chunk:  # End of file\n",
        "                    break\n",
        "                    \n",
        "                chunk_count += 1\n",
        "                total_chars += len(chunk)\n",
        "                \n",
        "                # Process the chunk (example: count characters)\n",
        "                if chunk_count % 10 == 0:  # Progress update\n",
        "                    print(f\"Processed {chunk_count} chunks, {total_chars} characters\")\n",
        "            \n",
        "            return total_chars, chunk_count\n",
        "            \n",
        "    except FileNotFoundError:\n",
        "        print(f\"File '{filename}' not found\")\n",
        "        return 0, 0\n",
        "\n",
        "# Create a sample \"large\" file for demonstration\n",
        "large_content = \"This is a sample line.\\n\" * 1000  # 1000 lines\n",
        "with open('large_sample.txt', 'w') as f:\n",
        "    f.write(large_content)\n",
        "\n",
        "# Process the file\n",
        "total_chars, chunks = process_large_file('large_sample.txt', chunk_size=100)\n",
        "print(f\"\\nFile processing complete:\")\n",
        "print(f\"Total characters: {total_chars:,}\")\n",
        "print(f\"Total chunks: {chunks}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Data Validation and Cleaning\n",
        "Always validate and clean data when reading from files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "def validate_user_data(row):\n",
        "    \"\"\"Validate and clean user data from CSV.\"\"\"\n",
        "    errors = []\n",
        "    \n",
        "    # Clean and validate name\n",
        "    name = row.get('Name', '').strip()\n",
        "    if not name:\n",
        "        errors.append('Name is required')\n",
        "    \n",
        "    # Validate email\n",
        "    email = row.get('Email', '').strip().lower()\n",
        "    if not email or '@' not in email:\n",
        "        errors.append('Valid email is required')\n",
        "    \n",
        "    # Validate age\n",
        "    try:\n",
        "        age = int(row.get('Age', 0))\n",
        "        if age < 0 or age > 150:\n",
        "            errors.append('Age must be between 0 and 150')\n",
        "    except ValueError:\n",
        "        errors.append('Age must be a number')\n",
        "        age = None\n",
        "    \n",
        "    return {\n",
        "        'name': name,\n",
        "        'email': email,\n",
        "        'age': age,\n",
        "        'errors': errors,\n",
        "        'valid': len(errors) == 0\n",
        "    }\n",
        "\n",
        "# Create sample data with some invalid entries\n",
        "sample_data = \"\"\"Name,Email,Age,City\n",
        "John Doe,john@example.com,25,New York\n",
        ",jane@example.com,30,Los Angeles\n",
        "Bob Johnson,invalid-email,35,Chicago\n",
        "Alice Brown,alice@example.com,999,Houston\n",
        "Charlie Wilson,charlie@example.com,28,Seattle\"\"\"\n",
        "\n",
        "with open('users_validation.csv', 'w') as f:\n",
        "    f.write(sample_data)\n",
        "\n",
        "# Process and validate the data\n",
        "valid_users = []\n",
        "invalid_users = []\n",
        "\n",
        "with open('users_validation.csv', 'r') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    \n",
        "    for row_num, row in enumerate(reader, 1):\n",
        "        validated = validate_user_data(row)\n",
        "        \n",
        "        if validated['valid']:\n",
        "            valid_users.append(validated)\n",
        "        else:\n",
        "            invalid_users.append((row_num, validated))\n",
        "\n",
        "print(f\"Data validation results:\")\n",
        "print(f\"Valid users: {len(valid_users)}\")\n",
        "print(f\"Invalid users: {len(invalid_users)}\")\n",
        "\n",
        "if invalid_users:\n",
        "    print(\"\\nValidation errors:\")\n",
        "    for row_num, user in invalid_users:\n",
        "        print(f\"Row {row_num}: {', '.join(user['errors'])}\")\n",
        "\n",
        "if valid_users:\n",
        "    print(\"\\nValid users:\")\n",
        "    for user in valid_users:\n",
        "        print(f\"  {user['name']} ({user['email']}) - Age {user['age']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Comparison\n",
        "\n",
        "### File Formats Comparison\n",
        "\n",
        "| Format | Best For | Pros | Cons |\n",
        "|--------|----------|------|------|\n",
        "| **Text (.txt)** | Simple data, logs, notes | Simple, universal, human-readable | No structure, manual parsing |\n",
        "| **CSV (.csv)** | Tabular data, spreadsheets | Structured, widely supported | Limited data types, no nesting |\n",
        "| **JSON (.json)** | APIs, configs, complex data | Structured, supports nesting, web standard | Larger file size, no comments |\n",
        "\n",
        "### Key Methods Summary\n",
        "\n",
        "| Operation | Text Files | CSV Files | JSON Files |\n",
        "|-----------|------------|-----------|------------|\n",
        "| **Read All** | `file.read()` | `csv.DictReader()` | `json.load()` |\n",
        "| **Read Lines** | `file.readlines()` | Loop through reader | N/A |\n",
        "| **Write** | `file.write()` | `csv.DictWriter()` | `json.dump()` |\n",
        "| **Append** | Mode `'a'` | Mode `'a'` + CSV writer | Load, modify, save |\n",
        "\n",
        "### Best Practices Checklist\n",
        "\n",
        "✅ **Always use `with` statements** for automatic file closing  \n",
        "✅ **Handle exceptions** gracefully with try/except  \n",
        "✅ **Validate data** when reading from external sources  \n",
        "✅ **Use appropriate file modes** (r, w, a)  \n",
        "✅ **Choose the right format** for your data type  \n",
        "✅ **Process large files** in chunks when needed  \n",
        "✅ **Use pathlib** for file path operations  \n",
        "✅ **Close files properly** (automatically with `with`)  \n",
        "\n",
        "## Next Steps\n",
        "\n",
        "1. **Practice with real data** - Try reading your own CSV or JSON files\n",
        "2. **Build a data processor** - Create a script that reads, processes, and writes data\n",
        "3. **Explore advanced topics** - Learn about binary files, file compression, and databases\n",
        "4. **Error handling** - Practice robust error handling for production code\n",
        "5. **Performance** - Learn about memory-efficient processing for large files\n",
        "\n",
        "File handling is fundamental to most real-world Python applications. Whether you're processing data, building web applications, or creating utilities, these skills will serve you well!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}